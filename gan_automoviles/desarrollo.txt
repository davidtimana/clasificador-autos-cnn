DESARROLLO

3.1 UTILIDAD PROFESIONAL DE LAS REDES GENERATIVAS ANTAGÓNICAS

Las Redes Generativas Antagónicas (GANs) representan una tecnología disruptiva que ha transformado significativamente el panorama de la inteligencia artificial y el procesamiento de imágenes. Su aplicación en el desempeño profesional abarca múltiples industrias y sectores, ofreciendo soluciones innovadoras a desafíos tradicionalmente complejos.

En el contexto del desarrollo de software y sistemas de inteligencia artificial, las GANs proporcionan herramientas poderosas para la generación de contenido sintético de alta calidad. Según Zafar (2018), estas redes han demostrado una capacidad excepcional para crear imágenes realistas que pueden ser utilizadas en diversos escenarios profesionales.

Las aplicaciones profesionales más relevantes incluyen:

• Diseño Automotriz: Las GANs pueden generar prototipos visuales de vehículos, permitiendo a los diseñadores explorar múltiples variaciones de forma rápida y eficiente.

• Simulación de Tráfico: En el contexto de sistemas de transporte inteligente, las GANs pueden generar escenarios de tráfico diversos para entrenar algoritmos de detección y clasificación de vehículos.

• Desarrollo de Videojuegos: La generación procedural de contenido visual mediante GANs permite crear entornos, personajes y objetos únicos.

• Análisis de Seguridad Vial: Las GANs pueden generar imágenes de accidentes simulados para entrenar sistemas de detección de riesgos.

• Realidad Aumentada y Virtual: La generación de contenido sintético es fundamental para crear experiencias inmersivas convincentes.

• Investigación Médica: En el campo de la medicina, las GANs pueden generar imágenes médicas sintéticas para entrenar algoritmos de diagnóstico.

3.2 ETAPA DE DISEÑO

3.2.1 Arquitectura de la Red Implementada

La arquitectura seleccionada para este proyecto es una Deep Convolutional Generative Adversarial Network (DCGAN), que combina las ventajas de las redes neuronales convolucionales tradicionales con el poder generativo de las GANs. Esta elección se fundamenta en las recomendaciones de Dadhich (2018), quien enfatiza la importancia de las capas convolucionales para el procesamiento eficiente de información espacial en imágenes.

3.2.1.1 Arquitectura del Generador

El Generador está diseñado para transformar un vector de ruido latente de 100 dimensiones en una imagen de 64x64 píxeles con 3 canales de color (RGB). La arquitectura utiliza capas de convolución transpuesta (ConvTranspose2d) que realizan un proceso de "upsampling" progresivo.

La estructura del Generador sigue el patrón:
• Entrada: Vector latente (100, 1, 1)
• Capa 1: ConvTranspose2d → (512, 4, 4) + BatchNorm + ReLU
• Capa 2: ConvTranspose2d → (256, 8, 8) + BatchNorm + ReLU
• Capa 3: ConvTranspose2d → (128, 16, 16) + BatchNorm + ReLU
• Capa 4: ConvTranspose2d → (64, 32, 32) + BatchNorm + ReLU
• Capa 5: ConvTranspose2d → (3, 64, 64) + Tanh

3.2.1.2 Arquitectura del Discriminador

El Discriminador está diseñado para clasificar imágenes como reales o generadas, utilizando una arquitectura convolucional que reduce progresivamente la resolución de la imagen mientras aumenta el número de canales de características.

La estructura sigue el patrón:
• Entrada: Imagen (3, 64, 64)
• Capa 1: Conv2d → (64, 32, 32) + LeakyReLU
• Capa 2: Conv2d → (128, 16, 16) + BatchNorm + LeakyReLU
• Capa 3: Conv2d → (256, 8, 8) + BatchNorm + LeakyReLU
• Capa 4: Conv2d → (512, 4, 4) + BatchNorm + LeakyReLU
• Capa 5: Conv2d → (1, 1, 1) + Sigmoid

3.2.2 Caracterización del Dataset

El dataset empleado en este proyecto es CIFAR-10, un conjunto de datos ampliamente utilizado en la comunidad de aprendizaje profundo que contiene 60,000 imágenes de 32x32 píxeles divididas en 10 clases. Para este proyecto específico, se ha filtrado el dataset para utilizar únicamente la categoría "automobile", que corresponde a la etiqueta 1 en la clasificación original de CIFAR-10.

Características del Dataset Filtrado:
• Número total de imágenes: 5,000 automóviles
• Resolución original: 32x32 píxeles
• Canales de color: 3 (RGB)
• Formato: Imágenes a color con fondo variado
• Variabilidad: Diferentes tipos de automóviles, ángulos de vista y condiciones de iluminación

3.3 ETAPA DE IMPLEMENTACIÓN

3.3.1 Funciones de Activación

Las funciones de activación seleccionadas han sido cuidadosamente elegidas siguiendo las mejores prácticas establecidas en la literatura sobre GANs.

3.3.1.1 ReLU (Rectified Linear Unit)

La función ReLU se utiliza en el Generador para las capas intermedias. Esta función se define como f(x) = max(0, x) y presenta varias ventajas:
• Eficiencia computacional: Es más rápida de calcular que funciones como tanh o sigmoid
• Evita el problema de saturación de gradientes: No satura para valores positivos
• Promueve la dispersión de características: Fomenta que las neuronas aprendan características diferentes

3.3.1.2 LeakyReLU

La función LeakyReLU se utiliza en el Discriminador y se define como f(x) = max(αx, x), donde α = 0.2. Esta función es particularmente importante en GANs porque:
• Permite el flujo de gradientes para valores negativos, evitando el problema de "dying ReLU"
• Mantiene la eficiencia computacional de ReLU
• Proporciona estabilidad durante el entrenamiento adversarial

3.3.1.3 Tanh (Tangente Hiperbólica)

La función Tanh se utiliza en la capa final del Generador y se define como f(x) = (e^x - e^(-x)) / (e^x + e^(-x)). Esta elección es crucial porque:
• Normaliza la salida al rango [-1, 1], compatible con las imágenes de entrenamiento normalizadas
• Proporciona gradientes más estables que sigmoid para valores cercanos a cero
• Es simétrica alrededor del origen, facilitando el entrenamiento

3.3.1.4 Sigmoid

La función Sigmoid se utiliza en la capa final del Discriminador y se define como f(x) = 1 / (1 + e^(-x)). Esta función es apropiada porque:
• Produce una probabilidad entre 0 y 1, interpretable como la confianza en que una imagen es real
• Es diferenciable en todo su dominio
• Proporciona una salida suave y continua

3.3.2 Técnicas de Regularización

3.3.2.1 Batch Normalization

La normalización por lotes se aplica en todas las capas intermedias del Generador y en las capas intermedias del Discriminador. Esta técnica:
• Normaliza las activaciones de cada capa, acelerando la convergencia
• Reduce la dependencia de la inicialización de pesos
• Permite el uso de tasas de aprendizaje más altas
• Estabiliza el entrenamiento adversarial

3.3.2.2 Inicialización de Pesos

Se implementa una inicialización específica para GANs:
• Pesos de capas convolucionales: Distribución normal con media 0 y desviación estándar 0.02
• Pesos de capas de normalización por lotes: Distribución normal con media 1 y desviación estándar 0.02
• Sesgos de capas de normalización por lotes: Inicializados en 0

3.3.3 Estrategias de Optimización

3.3.3.1 Optimizador Adam

Se utiliza el optimizador Adam con los siguientes hiperparámetros:
• Tasa de aprendizaje: 0.0002 (establecida como estándar para GANs)
• Beta1: 0.5 (reduce la influencia de gradientes pasados)
• Beta2: 0.999 (estándar para Adam)

3.3.3.2 Función de Pérdida

Se utiliza la función de pérdida de entropía cruzada binaria (BCELoss):
• Para el discriminador: Minimiza la clasificación incorrecta de imágenes reales y generadas
• Para el generador: Maximiza la probabilidad de que las imágenes generadas sean clasificadas como reales

3.3.4 Flujo de Datos en la Red

El flujo de datos en la DCGAN implementada sigue un patrón específico que optimiza el entrenamiento adversarial:

3.3.4.1 Fase de Entrenamiento del Discriminador

1. Se presenta un lote de imágenes reales del dataset
2. El discriminador procesa las imágenes reales y produce probabilidades
3. Se calcula la pérdida para imágenes reales (debe ser cercana a 1)
4. Se genera un lote de imágenes falsas usando el generador
5. El discriminador procesa las imágenes falsas y produce probabilidades
6. Se calcula la pérdida para imágenes falsas (debe ser cercana a 0)
7. Se combinan las pérdidas y se actualizan los pesos del discriminador

3.3.4.2 Fase de Entrenamiento del Generador

1. Se genera un lote de imágenes falsas usando el generador
2. El discriminador procesa las imágenes falsas
3. Se calcula la pérdida del generador (debe maximizar la probabilidad de que las imágenes falsas sean clasificadas como reales)
4. Se actualizan los pesos del generador

3.3.5 Implementación Paso a Paso del Código

3.3.5.1 Configuración Inicial

```python
CONFIG = {
    "batch_size": 64,       # Tamaño del lote optimizado
    "latent_dim": 100,      # Dimensión del espacio latente
    "lr": 0.0002,          # Tasa de aprendizaje estándar para GANs
    "beta1": 0.5,          # Parámetro beta1 para Adam
    "epochs": 50,          # Número de épocas de entrenamiento
    "num_final_images": 30, # Número de imágenes finales a generar
    "image_size": 64,      # Tamaño de imagen de salida
    "channels": 3          # Número de canales de color
}
```

3.3.5.2 Dataset Personalizado

```python
class CarDataset(Dataset):
    def __init__(self, transform=None):
        cifar_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform
        )
        # Filtra solo automóviles (etiqueta 1 en CIFAR-10)
        self.car_indices = [i for i, (_, label) in enumerate(cifar_dataset) if label == 1]
        self.cifar_dataset = cifar_dataset
```

3.3.5.3 Generador

```python
class Generator(nn.Module):
    def __init__(self, latent_dim, channels):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            # Capa 1: Vector latente -> (512, 4, 4)
            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            
            # Capa 2: (512, 4, 4) -> (256, 8, 8)
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            
            # Capas adicionales para 64x64...
            # Capa final con Tanh
            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),
            nn.Tanh()
        )
```

3.3.5.4 Discriminador

```python
class Discriminator(nn.Module):
    def __init__(self, channels):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # Capa 1: (3, 64, 64) -> (64, 32, 32)
            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            
            # Capas intermedias con BatchNorm
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # Capa final con Sigmoid
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )
```

3.3.5.5 Bucle de Entrenamiento

```python
for epoch in range(CONFIG["epochs"]):
    for i, data in enumerate(dataloader):
        # Entrenamiento del Discriminador
        netD.zero_grad()
        real_cpu = data.to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
        output = netD(real_cpu).view(-1)
        errD_real = criterion(output, label)
        errD_real.backward()
        
        # Entrenamiento con imágenes falsas
        noise = torch.randn(b_size, CONFIG["latent_dim"], 1, 1, device=device)
        fake = netG(noise)
        label.fill_(fake_label)
        output = netD(fake.detach()).view(-1)
        errD_fake = criterion(output, label)
        errD_fake.backward()
        errD = errD_real + errD_fake
        optimizerD.step()
        
        # Entrenamiento del Generador
        netG.zero_grad()
        label.fill_(real_label)
        output = netD(fake).view(-1)
        errG = criterion(output, label)
        errG.backward()
        optimizerG.step()
```

3.3.5.6 Generación de Imágenes Finales

```python
def generate_final_images(generator, latent_dim, num_images, device):
    generator.eval()
    with torch.no_grad():
        noise = torch.randn(num_images, latent_dim, 1, 1, device=device)
        final_images = generator(noise).detach().cpu()
    
    # Guardar cada imagen individualmente
    for i in range(num_images):
        img = final_images[i]
        torchvision.utils.save_image(img, f"results/final_generated/car_{i+1:02d}.png", normalize=True)
```

Esta implementación completa representa una aplicación práctica de los principios teóricos de las GANs, siguiendo las mejores prácticas establecidas en la literatura y adaptándose a las necesidades específicas del proyecto de generación de imágenes de automóviles.
