# -*- coding: utf-8 -*-
"""
CLASIFICADOR DE AUTOS CNN - DATASET CARS196
Google Colab Version
============================================

Este script est√° optimizado para ejecutarse en Google Colab.
Incluye todas las funcionalidades del clasificador de autos:
- Transfer Learning con ResNet50V2
- Data Augmentation
- Fine-tuning
- Evaluaci√≥n completa
- Soporte para im√°genes personalizadas

Autor: David Timana
Fecha: 2024
Curso: Visi√≥n por Computador
"""

# =============================================================================
# CONFIGURACI√ìN INICIAL PARA GOOGLE COLAB
# =============================================================================
import os
import sys
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n para Google Colab
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# =============================================================================
# INSTALACI√ìN DE DEPENDENCIAS (Google Colab)
# =============================================================================
print("üîß Instalando dependencias...")

# Instalar tensorflow-datasets si no est√° disponible
try:
    import tensorflow_datasets as tfds
except ImportError:
    print("Instalando tensorflow-datasets...")
    !pip install tensorflow-datasets

# =============================================================================
# IMPORTACI√ìN DE LIBRER√çAS
# =============================================================================
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.metrics import classification_report, confusion_matrix
import cv2
from PIL import Image
import glob
import zipfile
from google.colab import files
from google.colab import drive

print(f"‚úÖ TensorFlow {tf.__version__} cargado exitosamente")
print(f"üöÄ GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}")

# =============================================================================
# CONFIGURACI√ìN DE PAR√ÅMETROS
# =============================================================================
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 0.001
NUM_CLASSES = 196
TRAIN_SPLIT = 0.8
VALIDATION_SPLIT = 0.2

print(f"üìä Configuraci√≥n:")
print(f"   üñºÔ∏è  Tama√±o de imagen: {IMG_SIZE}x{IMG_SIZE}")
print(f"   üì¶ Batch size: {BATCH_SIZE}")
print(f"   üîÑ √âpocas: {EPOCHS}")
print(f"   üéØ Learning rate: {LEARNING_RATE}")
print(f"   üè∑Ô∏è  Clases: {NUM_CLASSES}")

# =============================================================================
# FUNCI√ìN 1: CARGA DEL DATASET CARS196
# =============================================================================
def cargar_dataset_cars196():
    """
    Carga el dataset Cars196 de TensorFlow Datasets
    """
    print("\nüîÑ Cargando dataset Cars196...")
    
    try:
        # Cargar dataset
        dataset, info = tfds.load('cars196', 
                                 with_info=True, 
                                 as_supervised=True,
                                 split=['train', 'test'])
        
        train_dataset, test_dataset = dataset[0], dataset[1]
        
        print(f"‚úÖ Dataset cargado exitosamente:")
        print(f"   üìä Entrenamiento: {info.splits['train'].num_examples:,} im√°genes")
        print(f"   üìä Prueba: {info.splits['test'].num_examples:,} im√°genes")
        print(f"   üè∑Ô∏è  Clases: {info.features['label'].num_classes}")
        print(f"   üìÅ Tama√±o total: ~1.82 GB")
        
        return train_dataset, test_dataset, info
        
    except Exception as e:
        print(f"‚ùå Error al cargar dataset: {e}")
        return None, None, None

# =============================================================================
# FUNCI√ìN 2: PREPROCESAMIENTO DE IM√ÅGENES
# =============================================================================
def preprocesar_imagen(image, label):
    """
    Preprocesa las im√°genes para el entrenamiento con data augmentation
    """
    # Redimensionar imagen
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
    
    # Normalizar valores de p√≠xeles (0-1)
    image = tf.cast(image, tf.float32) / 255.0
    
    # Data augmentation para entrenamiento
    if tf.random.uniform([]) > 0.5:
        image = tf.image.random_flip_left_right(image)
    
    if tf.random.uniform([]) > 0.5:
        image = tf.image.random_brightness(image, 0.2)
    
    if tf.random.uniform([]) > 0.5:
        image = tf.image.random_contrast(image, 0.8, 1.2)
    
    return image, label

def preprocesar_imagen_test(image, label):
    """
    Preprocesa las im√°genes para validaci√≥n/prueba (sin augmentation)
    """
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

# =============================================================================
# FUNCI√ìN 3: CREACI√ìN DEL MODELO CON TRANSFER LEARNING
# =============================================================================
def crear_modelo_transfer_learning():
    """
    Crea un modelo usando transfer learning con ResNet50V2
    """
    print("\nüöÄ Creando modelo con Transfer Learning (ResNet50V2)...")
    
    # Cargar modelo base pre-entrenado
    base_model = ResNet50V2(
        weights='imagenet',
        include_top=False,
        input_shape=(IMG_SIZE, IMG_SIZE, 3)
    )
    
    # Congelar las capas del modelo base
    base_model.trainable = False
    
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(NUM_CLASSES, activation='softmax')
    ])
    
    print("‚úÖ Modelo de transfer learning creado exitosamente")
    print(f"   üìä Par√°metros totales: {model.count_params():,}")
    
    return model, base_model

# =============================================================================
# FUNCI√ìN 4: ENTRENAMIENTO DEL MODELO
# =============================================================================
def entrenar_modelo(model, train_dataset, val_dataset, base_model=None):
    """
    Entrena el modelo con fine-tuning
    """
    print("\nüöÄ Iniciando entrenamiento...")
    
    # Compilar modelo
    model.compile(
        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy', 'top_5_accuracy']
    )
    
    # Callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_accuracy',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        ),
        ModelCheckpoint(
            'mejor_modelo_autos.h5',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # Entrenamiento inicial
    print("üìö Fase 1: Entrenamiento inicial...")
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=EPOCHS,
        callbacks=callbacks,
        verbose=1
    )
    
    # Fine-tuning si hay modelo base
    if base_model is not None:
        print("\nüîß Fase 2: Fine-tuning...")
        
        # Descongelar algunas capas del modelo base
        base_model.trainable = True
        
        # Congelar las primeras capas
        for layer in base_model.layers[:-30]:
            layer.trainable = False
        
        # Recompilar con learning rate m√°s bajo
        model.compile(
            optimizer=optimizers.Adam(learning_rate=LEARNING_RATE/10),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )
        
        # Fine-tuning
        history_fine = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=10,
            callbacks=callbacks,
            verbose=1
        )
        
        # Combinar historiales
        for key in history.history:
            history.history[key].extend(history_fine.history[key])
    
    print("‚úÖ Entrenamiento completado exitosamente")
    return history

# =============================================================================
# FUNCI√ìN 5: EVALUACI√ìN DEL MODELO
# =============================================================================
def evaluar_modelo(model, test_dataset, info):
    """
    Eval√∫a el modelo en el conjunto de prueba
    """
    print("\nüìä Evaluando modelo...")
    
    # Evaluaci√≥n
    test_loss, test_accuracy, test_top5_accuracy = model.evaluate(test_dataset, verbose=1)
    
    print(f"\nüìà Resultados de evaluaci√≥n:")
    print(f"   üìâ P√©rdida de prueba: {test_loss:.4f}")
    print(f"   ‚úÖ Precisi√≥n de prueba: {test_accuracy:.4f}")
    print(f"   üèÜ Top-5 precisi√≥n: {test_top5_accuracy:.4f}")
    
    return test_accuracy, test_top5_accuracy

# =============================================================================
# FUNCI√ìN 6: VISUALIZACI√ìN DE RESULTADOS
# =============================================================================
def visualizar_resultados(history):
    """
    Visualiza los resultados del entrenamiento
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Precisi√≥n
    axes[0, 0].plot(history.history['accuracy'], label='Entrenamiento')
    axes[0, 0].plot(history.history['val_accuracy'], label='Validaci√≥n')
    axes[0, 0].set_title('Precisi√≥n del Modelo')
    axes[0, 0].set_xlabel('√âpoca')
    axes[0, 0].set_ylabel('Precisi√≥n')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # P√©rdida
    axes[0, 1].plot(history.history['loss'], label='Entrenamiento')
    axes[0, 1].plot(history.history['val_loss'], label='Validaci√≥n')
    axes[0, 1].set_title('P√©rdida del Modelo')
    axes[0, 1].set_xlabel('√âpoca')
    axes[0, 1].set_ylabel('P√©rdida')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # Top-5 Precisi√≥n
    axes[1, 0].plot(history.history['top_5_accuracy'], label='Entrenamiento')
    axes[1, 0].plot(history.history['val_top_5_accuracy'], label='Validaci√≥n')
    axes[1, 0].set_title('Top-5 Precisi√≥n del Modelo')
    axes[1, 0].set_xlabel('√âpoca')
    axes[1, 0].set_ylabel('Top-5 Precisi√≥n')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    plt.tight_layout()
    plt.show()

# =============================================================================
# FUNCI√ìN 7: PREDICCI√ìN DE IM√ÅGENES PERSONALIZADAS
# =============================================================================
def predecir_imagen_personalizada(model, ruta_imagen):
    """
    Predice la clase de una imagen personalizada
    """
    # Cargar y preprocesar imagen
    img = load_img(ruta_imagen, target_size=(IMG_SIZE, IMG_SIZE))
    img_array = img_to_array(img)
    img_array = img_array / 255.0
    img_array = np.expand_dims(img_array, 0)
    
    # Predicci√≥n
    predictions = model.predict(img_array)
    predicted_class = np.argmax(predictions[0])
    confidence = np.max(predictions[0])
    
    # Top-5 predicciones
    top_5_indices = np.argsort(predictions[0])[-5:][::-1]
    top_5_confidences = predictions[0][top_5_indices]
    
    return predicted_class, confidence, top_5_indices, top_5_confidences

# =============================================================================
# FUNCI√ìN 8: SUBIR IM√ÅGENES PERSONALIZADAS (Google Colab)
# =============================================================================
def subir_imagenes_personalizadas():
    """
    Permite subir im√°genes personalizadas en Google Colab
    """
    print("\nüìÅ Subiendo im√°genes personalizadas...")
    
    # Crear carpeta para im√°genes
    if not os.path.exists('imagenes_personalizadas'):
        os.makedirs('imagenes_personalizadas')
    
    # Subir archivo ZIP con im√°genes
    print("üì§ Por favor, sube un archivo ZIP con tus im√°genes:")
    uploaded = files.upload()
    
    # Extraer im√°genes
    for filename in uploaded.keys():
        if filename.endswith('.zip'):
            with zipfile.ZipFile(filename, 'r') as zip_ref:
                zip_ref.extractall('imagenes_personalizadas')
            print(f"‚úÖ Im√°genes extra√≠das de {filename}")
    
    # Listar im√°genes
    imagenes = glob.glob('imagenes_personalizadas/**/*.jpg', recursive=True)
    imagenes.extend(glob.glob('imagenes_personalizadas/**/*.jpeg', recursive=True))
    imagenes.extend(glob.glob('imagenes_personalizadas/**/*.png', recursive=True))
    
    print(f"üìä Encontradas {len(imagenes)} im√°genes")
    return imagenes

# =============================================================================
# FUNCI√ìN 9: EVALUAR IM√ÅGENES PERSONALIZADAS
# =============================================================================
def evaluar_imagenes_personalizadas(model, imagenes):
    """
    Eval√∫a las im√°genes personalizadas
    """
    print(f"\nüîç Evaluando {len(imagenes)} im√°genes personalizadas...")
    
    resultados = []
    
    for i, ruta_imagen in enumerate(imagenes):
        try:
            clase_predicha, confianza, top5_indices, top5_confianzas = predecir_imagen_personalizada(model, ruta_imagen)
            
            resultado = {
                'imagen': os.path.basename(ruta_imagen),
                'clase_predicha': clase_predicha,
                'confianza': confianza,
                'top5_clases': top5_indices,
                'top5_confianzas': top5_confianzas
            }
            resultados.append(resultado)
            
            print(f"   {i+1:3d}. {os.path.basename(ruta_imagen):30s} | Clase: {clase_predicha:3d} | Conf: {confianza:.3f}")
            
        except Exception as e:
            print(f"   ‚ùå Error en {os.path.basename(ruta_imagen)}: {e}")
    
    return resultados

# =============================================================================
# FUNCI√ìN PRINCIPAL
# =============================================================================
def main():
    """
    Funci√≥n principal del clasificador
    """
    print("=" * 60)
    print("üöó CLASIFICADOR DE AUTOS CNN - DATASET CARS196")
    print("üìö Google Colab Version - Visi√≥n por Computador")
    print("=" * 60)
    
    # 1. Cargar dataset
    train_dataset, test_dataset, info = cargar_dataset_cars196()
    if train_dataset is None:
        print("‚ùå No se pudo cargar el dataset")
        return
    
    # 2. Preprocesar datos
    print("\nüîÑ Preprocesando datos...")
    
    # Aplicar preprocesamiento
    train_dataset = train_dataset.map(preprocesar_imagen, num_parallel_calls=tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocesar_imagen_test, num_parallel_calls=tf.data.AUTOTUNE)
    
    # Configurar para rendimiento
    train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    
    # Dividir train en train y validation
    train_size = int(TRAIN_SPLIT * info.splits['train'].num_examples)
    val_size = info.splits['train'].num_examples - train_size
    
    train_dataset_final = train_dataset.take(train_size // BATCH_SIZE)
    val_dataset = train_dataset.skip(train_size // BATCH_SIZE).take(val_size // BATCH_SIZE)
    
    print(f"‚úÖ Datos preprocesados:")
    print(f"   üìä Entrenamiento: {train_size:,} im√°genes")
    print(f"   üìä Validaci√≥n: {val_size:,} im√°genes")
    print(f"   üìä Prueba: {info.splits['test'].num_examples:,} im√°genes")
    
    # 3. Crear modelo
    model, base_model = crear_modelo_transfer_learning()
    model.summary()
    
    # 4. Entrenar modelo
    history = entrenar_modelo(model, train_dataset_final, val_dataset, base_model)
    if history is None:
        print("‚ùå Error durante el entrenamiento")
        return
    
    # 5. Evaluar modelo
    test_accuracy, test_top5_accuracy = evaluar_modelo(model, test_dataset, info)
    
    # 6. Visualizar resultados
    visualizar_resultados(history)
    
    # 7. Guardar modelo
    model.save('modelo_autos_final.h5')
    print("\nüíæ Modelo guardado como 'modelo_autos_final.h5'")
    
    # 8. Resumen final
    print(f"\n" + "=" * 60)
    print(f"üéâ ENTRENAMIENTO COMPLETADO EXITOSAMENTE")
    print(f"=" * 60)
    print(f"üìä Precisi√≥n final: {test_accuracy:.4f}")
    print(f"üèÜ Top-5 precisi√≥n: {test_top5_accuracy:.4f}")
    print(f"üéØ Modelo listo para usar con im√°genes personalizadas!")
    print(f"üìÅ Archivos generados:")
    print(f"   - modelo_autos_final.h5 (modelo final)")
    print(f"   - mejor_modelo_autos.h5 (mejor modelo durante entrenamiento)")
    
    # 9. Opci√≥n para evaluar im√°genes personalizadas
    print(f"\n" + "=" * 60)
    print(f"üîç EVALUACI√ìN DE IM√ÅGENES PERSONALIZADAS")
    print(f"=" * 60)
    
    evaluar_personalizadas = input("¬øDeseas evaluar im√°genes personalizadas? (s/n): ").lower().strip()
    
    if evaluar_personalizadas == 's':
        try:
            imagenes = subir_imagenes_personalizadas()
            if imagenes:
                resultados = evaluar_imagenes_personalizadas(model, imagenes)
                
                # Crear DataFrame con resultados
                df_resultados = pd.DataFrame([
                    {
                        'Imagen': r['imagen'],
                        'Clase_Predicha': r['clase_predicha'],
                        'Confianza': r['confianza'],
                        'Top1_Clase': r['top5_clases'][0],
                        'Top1_Confianza': r['top5_confianzas'][0]
                    }
                    for r in resultados
                ])
                
                print(f"\nüìä Resumen de predicciones:")
                print(df_resultados.head(10))
                
                # Guardar resultados
                df_resultados.to_csv('resultados_imagenes_personalizadas.csv', index=False)
                print(f"\nüíæ Resultados guardados en 'resultados_imagenes_personalizadas.csv'")
                
        except Exception as e:
            print(f"‚ùå Error al evaluar im√°genes personalizadas: {e}")
    
    print(f"\nüéâ ¬°Proyecto completado exitosamente!")

# =============================================================================
# EJECUCI√ìN DEL PROGRAMA
# =============================================================================
if __name__ == "__main__":
    main()
